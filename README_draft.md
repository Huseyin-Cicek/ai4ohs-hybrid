Here‚Äôs the agreed folder layout for **code**, **raw (Data Lake)**, and **clean (Data Warehouse)**. I‚Äôve fully expanded the **code** tree with practical submodules and config files so you can scaffold it 1:1.

# 1) CODE ‚Äî `C:\vscode-projects\ai4ohs-hybrid\`

```text
ai4ohs-hybrid/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ setup.cfg
‚îú‚îÄ‚îÄ .editorconfig
‚îú‚îÄ‚îÄ .pre-commit-config.yaml
‚îú‚îÄ‚îÄ .vscode/
‚îÇ   ‚îú‚îÄ‚îÄ extensions.json
‚îÇ   ‚îú‚îÄ‚îÄ settings.json
‚îÇ   ‚îú‚îÄ‚îÄ launch.json
‚îÇ   ‚îî‚îÄ‚îÄ tasks.json
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ dev/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ zeus_listener.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auto_ml_worker.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reorg_sanitizer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ register_zeus_startup.ps1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ zeus_ml_summary_example.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ startup/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ zeus_listener_startup.cmd
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ readme.txt
‚îÇ   ‚îî‚îÄ‚îÄ tools/
‚îÇ       ‚îú‚îÄ‚îÄ backup_dataset.py
‚îÇ       ‚îú‚îÄ‚îÄ validate_tree.py
‚îÇ       ‚îî‚îÄ‚îÄ check_md_links.py
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ ai4ohs-hybrid-roadmap.md
‚îÇ   ‚îú‚îÄ‚îÄ ai4ohs-hybrid-system-instructions.md
‚îÇ   ‚îú‚îÄ‚îÄ ai4ohs-hybrid-rag-cag-instructions.txt
‚îÇ   ‚îú‚îÄ‚îÄ ai4ohs_data_flow_pipeline.md
‚îÇ   ‚îú‚îÄ‚îÄ zeus_listener.md
‚îÇ   ‚îú‚îÄ‚îÄ ffmp.md
‚îÇ   ‚îú‚îÄ‚îÄ faiss.md
‚îÇ   ‚îî‚îÄ‚îÄ secrets.md
‚îú‚îÄ‚îÄ logs/                          # git-ignored (runtime)
‚îÇ   ‚îú‚îÄ‚îÄ dev/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îî‚îÄ‚îÄ tools/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py            # OFFLINE_MODE, paths, model names
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ paths.py               # central path resolver (code/raw/clean)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging_conf.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ documents.py       # Pydantic models (DocMeta, DocChunk‚Ä¶)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ datasets.py        # Dataset schemas (incidents, NCR, SOF‚Ä¶)
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_extract.py        # pdf/docx/ocr dispatchers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ocr.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cleaners.py            # unicode/whitespace/boilerplate removal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ splitters.py           # rule-based & token-aware splitting
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hashing.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ files.py               # safe fs ops, sanitizer, dedupe
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ faiss_index.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reranker.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search.py              # retriever + hybrid scoring
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ compliance.py          # CAG guardrail rules
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ wb_ifc_mappers.py      # ESS/EHS mapping helpers
‚îÇ   ‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 00_ingest/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run.py             # entrypoint
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loaders/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fs_loader.py   # local FS, mail backups, OneDrive exports
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_loader.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docx_loader.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image_loader.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ excel_loader.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validators/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mime_checks.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ filename_policy.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ parsers/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ pdf_parser.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ docx_parser.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ image_ocr_parser.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_staging/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ normalizers/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_normalizer.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata_normalizer.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enrichers/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ language_detect.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ date_location_tagging.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ qc/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ staging_validations.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02_processing/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunking/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rules.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ build_embeddings.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py      # sentence-transformers config
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ build_faiss.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index_layout.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stats/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ dataset_stats.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 03_rag/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retriever/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bm25_hybrid.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reranker/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cross_encoder.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ eval/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ rag_eval.py
‚îÇ   ‚îî‚îÄ‚îÄ ohs/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ api/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ main.py            # FastAPI app (uvicorn entrypoint)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ routers/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ health.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search.py      # /search?q=... (RAG)
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ guardrails.py  # /validate (CAG checks)
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ datasets.py    # list/query indexed datasets
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ request.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ response.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ deps/
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ containers.py  # DI wiring, settings, logger
‚îÇ       ‚îî‚îÄ‚îÄ services/
‚îÇ           ‚îú‚îÄ‚îÄ __init__.py
‚îÇ           ‚îú‚îÄ‚îÄ rag_service.py
‚îÇ           ‚îú‚îÄ‚îÄ cag_service.py
‚îÇ           ‚îî‚îÄ‚îÄ logging_service.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_cleaners.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_splitters.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_embeddings.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_faiss_index.py
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ       ‚îú‚îÄ‚îÄ test_health.py
‚îÇ       ‚îú‚îÄ‚îÄ test_search.py
‚îÇ       ‚îî‚îÄ‚îÄ test_guardrails.py
‚îî‚îÄ‚îÄ examples/
    ‚îú‚îÄ‚îÄ curl_search.http
    ‚îî‚îÄ‚îÄ postman_collection.json
```

**Notes (code):**

* **Entrypoints**

  * Pipelines:

    ```powershell
    python src/pipelines/00_ingest/run.py
    python src/pipelines/01_staging/run.py
    python src/pipelines/02_processing/run.py
    python src/pipelines/03_rag/run.py
    ```
  * API:

    ```powershell
    uvicorn src.ohs.api.main:app --reload --host 127.0.0.1 --port 8000
    ```
* **Key env toggles** in `src/config/settings.py`: `OFFLINE_MODE`, `EMBEDDING_MODEL`, `RERANKER_MODEL`, `GPU_ACCELERATION`, base paths for **raw/clean**.
* **.env.example** should define `RAW_ROOT`, `CLEAN_ROOT`, `LOG_ROOT`, `OFFLINE_MODE`, etc.
* **logs/** stays Git-ignored.
* **pre-commit** handles Ruff/Black/Isort and MD link checks.

---

# 2) RAW ‚Äî Data Lake ‚Äî `H:\DataLake\ai4ohs-hybrid-datasets-raw\`

```text
ai4ohs-hybrid-datasets-raw/
‚îú‚îÄ‚îÄ _archive/                                # long-term frozen sources
‚îÇ   ‚îú‚îÄ‚îÄ 2024/
‚îÇ   ‚îî‚îÄ‚îÄ 2025/
‚îú‚îÄ‚îÄ _system/
‚îÇ   ‚îú‚îÄ‚îÄ _bin/
‚îÇ   ‚îú‚îÄ‚îÄ _cache/
‚îÇ   ‚îú‚îÄ‚îÄ _inventory/                          # file inventory (parquet/csv)
‚îÇ   ‚îú‚îÄ‚îÄ _reports/
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ backup/
‚îú‚îÄ‚îÄ 00_sources/
‚îÇ   ‚îú‚îÄ‚îÄ mailbackup/
‚îÇ   ‚îú‚îÄ‚îÄ onedrive_local/
‚îÇ   ‚îú‚îÄ‚îÄ hsse-docs/
‚îÇ   ‚îú‚îÄ‚îÄ isg-docs/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ video/
‚îÇ   ‚îî‚îÄ‚îÄ _dropzone/                           # hot folder (FFMP sanitizes)
‚îú‚îÄ‚îÄ 01_staging/
‚îÇ   ‚îú‚îÄ‚îÄ _workdir_tmp/
‚îÇ   ‚îú‚îÄ‚îÄ normalized_text/
‚îÇ   ‚îú‚îÄ‚îÄ ocr/
‚îÇ   ‚îú‚îÄ‚îÄ metadata/                            # extracted front-matter JSON
‚îÇ   ‚îî‚îÄ‚îÄ qc_reports/
‚îî‚îÄ‚îÄ 02_processing/
    ‚îú‚îÄ‚îÄ chunks/
    ‚îú‚îÄ‚îÄ embeddings/
    ‚îú‚îÄ‚îÄ faiss/
    ‚îú‚îÄ‚îÄ hybrid_indexes/
    ‚îî‚îÄ‚îÄ stats/
```

**Notes (raw):**

* **FFMP** writes sanitized copies from `00_sources/_dropzone` ‚Üí into proper subfolders.
* `01_staging` holds normalized text & OCR outputs with per-file `*.meta.json`.

---

# 3) CLEAN ‚Äî Data Warehouse ‚Äî `H:\DataWarehouse\ai4ohs-datasets-clean\`

```text
ai4ohs-datasets-clean/
‚îú‚îÄ‚îÄ dims/                                     # slowly changing dims
‚îÇ   ‚îú‚îÄ‚îÄ dim_projects.parquet
‚îÇ   ‚îú‚îÄ‚îÄ dim_contractors.parquet
‚îÇ   ‚îú‚îÄ‚îÄ dim_locations.parquet
‚îÇ   ‚îú‚îÄ‚îÄ dim_assets.parquet
‚îÇ   ‚îî‚îÄ‚îÄ dim_workers.parquet
‚îú‚îÄ‚îÄ facts/
‚îÇ   ‚îú‚îÄ‚îÄ fact_incidents_daily.parquet
‚îÇ   ‚îú‚îÄ‚îÄ fact_near_miss.parquet
‚îÇ   ‚îú‚îÄ‚îÄ fact_ncr.parquet
‚îÇ   ‚îú‚îÄ‚îÄ fact_sof.parquet
‚îÇ   ‚îú‚îÄ‚îÄ fact_training_hours.parquet
‚îÇ   ‚îî‚îÄ‚îÄ fact_kpi_monthly.parquet
‚îú‚îÄ‚îÄ entities/                                  # curated entity tables
‚îÇ   ‚îú‚îÄ‚îÄ incidents/
‚îÇ   ‚îú‚îÄ‚îÄ ncr/
‚îÇ   ‚îú‚îÄ‚îÄ sof/
‚îÇ   ‚îú‚îÄ‚îÄ permits/
‚îÇ   ‚îú‚îÄ‚îÄ audits/
‚îÇ   ‚îî‚îÄ‚îÄ trainings/
‚îú‚îÄ‚îÄ marts/                                     # reporting-ready views
‚îÇ   ‚îú‚îÄ‚îÄ wb_ifc_esmf_annex9/
‚îÇ   ‚îú‚îÄ‚îÄ iso_45001_dash/
‚îÇ   ‚îî‚îÄ‚îÄ exec_summary/
‚îú‚îÄ‚îÄ exports/
‚îÇ   ‚îú‚îÄ‚îÄ excel/
‚îÇ   ‚îú‚îÄ‚îÄ csv/
‚îÇ   ‚îî‚îÄ‚îÄ json/
‚îî‚îÄ‚îÄ backup/
```

**Notes (clean):**

* **facts/** and **dims/** are the canonical star-schema layers for reporting.
* **marts/** aligns directly to deliverables (e.g., ESMF Annex 9, ILBANK dashboards).

---

## Suggested `.gitignore` (key lines)

```gitignore
# Python
.venv/
__pycache__/
*.pyc

# Local config
.env
logs/
*.log
*.sqlite

# Data (never commit)
H:/DataLake/
H:/DataWarehouse/

# Tools
.vscode/.ropeproject/
```

## Quick Scaffolding (PowerShell)

```powershell
# run from C:\vscode-projects
mkdir ai4ohs-hybrid, cd ai4ohs-hybrid
# create folders
"scripts/dev","scripts/tools",".vscode","docs","logs","tests/unit","tests/api",
"src","src/config/schemas","src/utils","src/pipelines/00_ingest/loaders",
"src/pipelines/00_ingest/validators","src/pipelines/00_ingest/parsers",
"src/pipelines/01_staging/normalizers","src/pipelines/01_staging/enrichers","src/pipelines/01_staging/qc",
"src/pipelines/02_processing/chunking","src/pipelines/02_processing/embedding","src/pipelines/02_processing/index","src/pipelines/02_processing/stats",
"src/pipelines/03_rag/retriever","src/pipelines/03_rag/reranker","src/pipelines/03_rag/eval",
"src/ohs/api/routers","src/ohs/api/models","src/ohs/api/deps","src/ohs/services","examples" |
% { mkdir $_ -Force } | Out-Null
# touch common files
"README.md",".gitignore",".env.example","requirements.txt","pyproject.toml","setup.cfg",".editorconfig",".pre-commit-config.yaml" |
% { ni $_ -ItemType File -Force } | Out-Null
```

## README.md
# AI4OHS-HYBRID ‚Äî Dual-Mode OHS Intelligence Engine
RAG + CAG + Compliance Guardrails + Zeus Automation Layer

---

### Overview
AI4OHS-HYBRID is a dual-mode (offline/online) intelligent Occupational Health & Safety system
designed for high-compliance infrastructure projects (WB / IFC ESS, ISO 45001, OSHA, 6331).

| Layer | Purpose |
|-------|----------|
| **RAG** | Local + cloud-optional semantic retrieval across OHS datasets |
| **CAG** | Compliance-Augmented Generation ‚Äî rule-based legal & standard validation |
| **ETL Pipelines** | Ingest ‚Üí Staging ‚Üí Processing ‚Üí RAG Index |
| **Zeus Automation** | Local voice trigger, file sanitizer, task scheduler |
| **Dual Execution** | Works 100 % offline or cloud-assisted |

---

### Quick Start
```bash
# 1Ô∏è‚É£ Install
python -m venv .venv && .\.venv\Scripts\activate
pip install -r requirements.txt

# 2Ô∏è‚É£ Run Pipelines
python src/pipelines/00_ingest/run.py
python src/pipelines/01_staging/run.py
python src/pipelines/02_processing/run.py
python src/pipelines/03_rag/run.py

# 3Ô∏è‚É£ Start API
uvicorn src.ohs.api.main:app --reload --host 127.0.0.1 --port 8000

Core Paths

Raw Data Lake: H:\DataLake\ai4ohs-hybrid-datasets-raw\

Clean Data Warehouse: H:\DataWarehouse\ai4ohs-datasets-clean\

Logs: logs\ (auto-excluded from Git)

Key Environment Flags
Variable	Description
OFFLINE_MODE	"true" = no remote model downloads
EMBEDDING_MODEL	Sentence Transformer used for RAG
RERANKER_MODEL	Cross-encoder reranker
GPU_ACCELERATION	Enable CUDA if available
Compliance Guardrails

Enforced via rule sets from ISO 45001, OSHA 29 CFR, Law 6331, and WB/IFC ESS.

Unsafe or non-regulatory recommendations are auto-blocked.

Developer Shortcuts
Command	Description
task: Run Pipelines	Runs all ETL stages sequentially
task: Start API	Launch FastAPI server
task: Format Code	Ruff + Black + Isort
task: Docs Check	Markdown link validation

¬© 2025 AI4OHS-HYBRID Project | OHS Compliance & Automation Framework


---

## ‚öôÔ∏è `.gitignore`
```gitignore
# Python
__pycache__/
*.pyc
*.pyo
*.pyd
*.egg-info/
.venv/
.env
env/
venv/

# Logs & caches
logs/
*.log
.cache/
__snapshots__/

# Data paths
H:/DataLake/ #raw data
H:/DataWarehouse/ #processed data

# VS Code
.vscode/.ropeproject/
.vscode/ipynb_checkpoints/

# OS
.DS_Store
Thumbs.db

üîë .env.example
# === Core Paths ===
RAW_ROOT=H:\DataLake\ai4ohs-hybrid-datasets-raw
CLEAN_ROOT=H:\DataWarehouse\ai4ohs-datasets-clean
LOG_ROOT=C:\vscode-projects\ai4ohs-hybrid\logs

# === Runtime ===
OFFLINE_MODE=true
GPU_ACCELERATION=true

# === Models ===
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L12-v2
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# === API ===
API_HOST=127.0.0.1
API_PORT=8000

üßæ requirements.txt
fastapi
uvicorn
pydantic
python-dotenv
sentence-transformers
faiss-cpu
rank-bm25
numpy
pandas
openpyxl
python-docx
pdfplumber
pytesseract
pillow
scikit-learn
loguru
black
isort
ruff
pre-commit
jinja2
pyyaml
watchdog

üß± pyproject.toml
[build-system]
requires = ["setuptools", "wheel"]

[tool.black]
line-length = 100
target-version = ['py310']

[tool.isort]
profile = "black"
line_length = 100
multi_line_output = 3

[tool.ruff]
select = ["E", "F", "I"]
ignore = ["E501"]
line-length = 100

[tool.coverage.run]
branch = true
source = ["src"]

[tool.coverage.report]
show_missing = true

‚öôÔ∏è setup.cfg
[metadata]
name = ai4ohs-hybrid
version = 2025.11.0
description = Dual-Mode OHS Intelligence Engine (RAG + CAG)
author = AI4OHS-HYBRID Team
license = MIT

[options]
packages = find:
install_requires =
    fastapi
    uvicorn
    pydantic
python_requires = >=3.10

[flake8]
max-line-length = 100
exclude = .venv,logs,__pycache__

‚úèÔ∏è .editorconfig
root = true

[*]
charset = utf-8
indent_style = space
indent_size = 4
end_of_line = lf
insert_final_newline = true
trim_trailing_whitespace = true

üßπ .pre-commit-config.yaml
repos:
  - repo: https://github.com/psf/black
    rev: 24.10.0
    hooks:
      - id: black
  - repo: https://github.com/charliermarsh/ruff-pre-commit
    rev: v0.6.9
    hooks:
      - id: ruff
        args: [--fix]
  - repo: https://github.com/pre-commit/mirrors-isort
    rev: v5.13.2
    hooks:
      - id: isort
  - repo: local
    hooks:
      - id: check-md-links
        name: Markdown Link Checker
        entry: python scripts/tools/check_md_links.py
        language: system
        files: \.md$

üíª .vscode/settings.json
{
    "python.defaultInterpreterPath": "${workspaceFolder}/.venv/Scripts/python.exe",
    "python.terminal.activateEnvironment": true,
    "python.formatting.provider": "black",
    "editor.formatOnSave": true,
    "editor.minimap.enabled": false,
    "editor.wordWrap": "on",
    "editor.rulers": [100],
    "files.trimTrailingWhitespace": true,
    "files.insertFinalNewline": true,
    "terminal.integrated.shellIntegration.enabled": true,
    "python.linting.enabled": true,
    "python.linting.ruffEnabled": true,
    "notebook.formatOnSave.enabled": true,
    "notebook.cellToolbarVisibility": "hover",
    "notebook.output.textLineLimit": 50,
    "notebook.output.wordWrap": "on"
}

üß≠ .vscode/launch.json
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Run FastAPI (AI4OHS-HYBRID)",
            "type": "python",
            "request": "launch",
            "module": "uvicorn",
            "args": [
                "src.ohs.api.main:app",
                "--reload",
                "--host", "127.0.0.1",
                "--port", "8000"
            ],
            "jinja": true,
            "cwd": "${workspaceFolder}"
        },
        {
            "name": "Run Pipeline 00‚Üí03",
            "type": "python",
            "request": "launch",
            "program": "${workspaceFolder}/scripts/dev/run_all_pipelines.py"
        }
    ]
}

‚öôÔ∏è .vscode/tasks.json
{
    "version": "2.0.0",
    "tasks": [
        {
            "label": "Run Pipelines",
            "type": "shell",
            "command": "python src/pipelines/00_ingest/run.py && python src/pipelines/01_staging/run.py && python src/pipelines/02_processing/run.py && python src/pipelines/03_rag/run.py",
            "group": { "kind": "build", "isDefault": true },
            "presentation": { "reveal": "always" },
            "problemMatcher": []
        },
        {
            "label": "Start API",
            "type": "shell",
            "command": "uvicorn src.ohs.api.main:app --reload --host 127.0.0.1 --port 8000",
            "group": "test",
            "presentation": { "reveal": "always" }
        },
        {
            "label": "Format Code",
            "type": "shell",
            "command": "ruff check . --fix && black . && isort .",
            "group": "none"
        },
        {
            "label": "Docs Check",
            "type": "shell",
            "command": "python scripts/tools/check_md_links.py",
            "group": "none"
        }
    ]
}

## scripts/dev/run_all_pipelines.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
AI4OHS-HYBRID ‚Äî Full Pipeline Runner
------------------------------------
Sequentially runs the ETL stages:
00_ingest ‚Üí 01_staging ‚Üí 02_processing ‚Üí 03_rag

Usage:
    python scripts/dev/run_all_pipelines.py

The script logs start/end time, captures exceptions, and
writes results to logs/pipelines/run_all.log.
"""

import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path

# === PATH SETUP ===
ROOT_DIR = Path(__file__).resolve().parents[2]
LOG_DIR = ROOT_DIR / "logs" / "pipelines"
LOG_DIR.mkdir(parents=True, exist_ok=True)
LOG_FILE = LOG_DIR / "run_all.log"

PIPELINE_ORDER = [
    "src/pipelines/00_ingest/run.py",
    "src/pipelines/01_staging/run.py",
    "src/pipelines/02_processing/run.py",
    "src/pipelines/03_rag/run.py",
]


def log(msg: str):
    """Write both to stdout and log file."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    formatted = f"[{timestamp}] {msg}"
    print(formatted)
    with LOG_FILE.open("a", encoding="utf-8") as f:
        f.write(formatted + "\n")


def run_stage(stage_path: str) -> bool:
    """Run a pipeline stage with subprocess."""
    log(f"‚ñ∂ Starting stage: {stage_path}")
    start = time.time()
    try:
        subprocess.run([sys.executable, stage_path], check=True)
        elapsed = round(time.time() - start, 2)
        log(f"‚úÖ Completed {stage_path} in {elapsed}s")
        return True
    except subprocess.CalledProcessError as e:
        log(f"‚ùå ERROR in {stage_path}: {e}")
        return False


def main():
    log("=" * 60)
    log("üöÄ AI4OHS-HYBRID ‚Äî Full Pipeline Run START")
    log(f"Working directory: {ROOT_DIR}")
    log("=" * 60)

    overall_start = time.time()
    success_count = 0

    for stage in PIPELINE_ORDER:
        if run_stage(stage):
            success_count += 1
        else:
            log("‚ö†Ô∏è Skipping remaining stages due to error.")
            break

    total_time = round(time.time() - overall_start, 2)
    log("-" * 60)
    log(f"üèÅ Finished {success_count}/{len(PIPELINE_ORDER)} stages in {total_time}s")
    log("=" * 60 + "\n")


if __name__ == "__main__":
    main()

üîç Features

Sequential, fail-fast execution (stops on first failure).

Real-time console output + log file at logs/pipelines/run_all.log.

Timestamped logging for reproducible ETL audits.

Compatible with VS Code launch.json and tasks.json.

Cross-platform (Windows PowerShell / Linux bash).

‚úÖ Example Output
[2025-11-09 21:57:10] üöÄ AI4OHS-HYBRID ‚Äî Full Pipeline Run START
[2025-11-09 21:57:10] ‚ñ∂ Starting stage: src/pipelines/00_ingest/run.py
[2025-11-09 21:57:15] ‚úÖ Completed src/pipelines/00_ingest/run.py in 4.8s
[2025-11-09 21:57:15] ‚ñ∂ Starting stage: src/pipelines/01_staging/run.py
[2025-11-09 21:57:23] ‚úÖ Completed src/pipelines/01_staging/run.py in 7.5s
[2025-11-09 21:57:23] ‚ñ∂ Starting stage: src/pipelines/02_processing/run.py
[2025-11-09 21:57:41] ‚úÖ Completed src/pipelines/02_processing/run.py in 18.0s
[2025-11-09 21:57:41] ‚ñ∂ Starting stage: src/pipelines/03_rag/run.py
[2025-11-09 21:57:50] ‚úÖ Completed src/pipelines/03_rag/run.py in 8.8s
[2025-11-09 21:57:50] üèÅ Finished 4/4 stages in 39.1s

## scripts/dev/run_all_pipelines.ps1

<#
.SYNOPSIS
    AI4OHS-HYBRID ‚Äî Full Pipeline Runner (PowerShell launcher)

.DESCRIPTION
    Launches scripts/dev/run_all_pipelines.py with robust logging, colorized output,
    and proper exit codes for Windows Task Scheduler and CI.

.USAGE
    pwsh -File scripts/dev/run_all_pipelines.ps1
    powershell -ExecutionPolicy Bypass -File scripts/dev/run_all_pipelines.ps1

.NOTES
    - Prefers .venv\Scripts\python.exe if present, otherwise falls back to system "python".
    - Transcript logs are saved under logs\pipelines\ with a timestamped filename.
#>

# Strict mode and fail-fast behavior
Set-StrictMode -Version Latest
$ErrorActionPreference = 'Stop'

# ===== Path resolution (repo-root aware) =====
$devDir     = Split-Path -Parent $MyInvocation.MyCommand.Path         # ...\scripts\dev
$scriptsDir = Split-Path -Parent $devDir                               # ...\scripts
$repoRoot   = Split-Path -Parent $scriptsDir                           # repo root
$runnerPy   = Join-Path $devDir 'run_all_pipelines.py'
$logsDir    = Join-Path $repoRoot 'logs\pipelines'

# Ensure logs directory exists
New-Item -ItemType Directory -Path $logsDir -Force | Out-Null

# Timestamped transcript file
$stamp          = Get-Date -Format 'yyyyMMdd_HHmmss'
$transcriptPath = Join-Path $logsDir "run_all_ps1_$stamp.log"

# ===== Environment hardening =====
# Make sure Python uses UTF-8 for consistent logs
$env:PYTHONUTF8      = '1'
$env:PYTHONIOENCODING = 'utf-8'

# Prefer local venv python if available
$venvPython = Join-Path $repoRoot '.venv\Scripts\python.exe'
if (Test-Path $venvPython) {
    $pythonExe = $venvPython
} else {
    $pythonExe = 'python'
}

# Basic banner
Write-Host ('=' * 70)
Write-Host "AI4OHS-HYBRID ‚Äî Pipeline Launcher (PowerShell)" -ForegroundColor Cyan
Write-Host "Repo Root     : $repoRoot"
Write-Host "Python        : $pythonExe"
Write-Host "Runner        : $runnerPy"
Write-Host "Transcript Log: $transcriptPath"
Write-Host ('=' * 70)

# Validate runner presence
if (-not (Test-Path $runnerPy)) {
    Write-Host "ERROR: Runner script not found: $runnerPy" -ForegroundColor Red
    exit 1
}

# Start transcript (best-effort; still proceed if it fails)
$transcriptStarted = $false
try {
    Start-Transcript -Path $transcriptPath -Force -ErrorAction Stop | Out-Null
    $transcriptStarted = $true
} catch {
    Write-Host "WARN: Could not start transcript: $($_.Exception.Message)" -ForegroundColor Yellow
}

# Execute the Python orchestrator
$overallStart = Get-Date
try {
    Write-Host "[START] $(Get-Date -Format 'yyyy-MM-dd HH:mm:ss') ‚Äî Running full ETL (00‚Üí03)..." -ForegroundColor Green

    & $pythonExe $runnerPy
    $exitCode = $LASTEXITCODE

    if ($exitCode -ne 0) {
        Write-Host "[FAIL] run_all_pipelines.py exited with code $exitCode" -ForegroundColor Red
        if ($transcriptStarted) {
            try { Stop-Transcript | Out-Null } catch {}
        }
        exit $exitCode
    }

    $elapsed = New-TimeSpan -Start $overallStart -End (Get-Date)
    $elapsedMsg = "{0:hh\:mm\:ss}" -f $elapsed
    Write-Host "[DONE] All stages completed successfully in $elapsedMsg" -ForegroundColor Green

    if ($transcriptStarted) {
        try { Stop-Transcript | Out-Null } catch {}
    }

    exit 0
}
catch {
    $err = $_.Exception
    Write-Host "EXCEPTION: $($err.GetType().FullName)" -ForegroundColor Red
    Write-Host "MESSAGE  : $($err.Message)" -ForegroundColor Red
    if ($err.StackTrace) {
        Write-Host "STACKTRACE:" -ForegroundColor DarkGray
        Write-Host $err.StackTrace -ForegroundColor DarkGray
    }

    if ($transcriptStarted) {
        try { Stop-Transcript | Out-Null } catch {}
    }
    exit 1
}

######################
Tip (first run): If you hit execution policy blocks, run this once in an elevated PowerShell:

Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy Bypass -Force
