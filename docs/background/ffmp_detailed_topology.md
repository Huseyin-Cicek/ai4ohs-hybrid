# File and Folder Management Procedure (FFMP)

## Purpose

The FFMP standardizes how source data, cleaned datasets, and processed ML artifacts are named, stored, and versioned. It ensures consistent ingestion into the AI4OHS-HYBRID pipelines while keeping regulatory and auditing requirements traceable.

## Directory Topology

### DataLake

```
H:\DataLake
├── ai4crypto-raw
│   ├── raw
│   │   ├── binance
│   │   │   ├── spot
│   │   │   │   └── 1m\HOTUSDT\year=2025\month=10\day=12
│   │   │   └── futures
│   │   │       ├── 1m\HOTUSDT\year=2025\month=10\day=12
│   │   │       └── funding\8h\HOTUSDT\year=2025\month=10
│   │   └── external
│   ├── curated
│   │   ├── datasets\train_test_splits
│   │   ├── datasets\walk_forward\wf_180_30\window_2024-01-01_2024-06-29\{train,test}
│   │   ├── features
│   │   ├── labels
│   │   └── ohlcv_aligned\{spot,futures}
│   ├── interim\{funding,ohlcv,orderbook}
│   ├── audits\{checksums,policy,schema_versions}
│   ├── backups\{export,snapshots}
│   ├── metadata\{checkpoints,schema_registry,source_manifest}
│   └── logs\{ingestion,validation}
└── ai4hsse-raw
        |── _cache
		├── _archive\{2025Q3,2025Q4}
		├── _system
		│   ├── _bin
		│   ├── _cache
		│   ├── _inventory
		│   ├── _logs
		│   ├── _reports
		│   ├── backup\{daily,weekly,monthly}
		│   ├── config
		│   ├── logs
		│   └── ollama_cache
		├── 00_sources
		│   ├── _dropzone
		│   ├── hsse-docs
		│   ├── images
		│   └── terrp-docs
		├── 01_staging
		│   ├── _workdir_tmp
		│   ├── metadata
		│   ├── normalized_text
		│   └── ocr
		└── 02_processing
				├── embeddings_raw
				└── faiss_build_input
```

### DataWarehouse

```
H:\DataWarehouse
├── ai4crypto-clean
└── ai4hsse-clean
		├── analytics\{incident_stats,kpis}
		├── cag_ready
		├── documents_parquet
		├── exports\{handoffs,reports}
		├── faiss
		├── images_ref
		├── onedrive-clean
		│   └── huseyincicek
		│       ├── consultants\ADANA_C1_ALTER\...
		│       ├── contractors\ADANA_W1_MERKEZ_WSP_\...
		│       ├── ilbank\{agreement,FrameWork}
		│       └── referances\{category,guides,standarts,...}
		├── processed
		│   ├── consultants\ADANA_C1_ALTER\...
		│   ├── contractors\ADANA_W1_MERKEZ_WSP_\...
		│   ├── ilbank\{agreement,FrameWork}
		│   └── referances\{category,guides,wb-ifc-ess,...}
		└── rag_ready\chunks_parquet
```

### Key Areas

- **DataLake\ai4hsse-raw\00_sources\***: Primary HSSE raw landing zones. `_dropzone` is fed by operators, while `hsse-docs`, `images`, and `terrp-docs` receive sanitized payloads per MIME type.
- **DataLake\ai4hsse-raw\01_staging\***: Intermediate OCR outputs, normalized text, and metadata managed by the 01_staging pipeline.
- **DataLake\ai4hsse-raw\02_processing\***: Embedding-ready corpora and FAISS build inputs prior to warehouse promotion.
- **DataLake\ai4crypto-raw\***: Financial market telemetry organized by lifecycle (`raw`, `interim`, `curated`), each with audits, backups, and metadata registries.
- **DataWarehouse\ai4hsse-clean\onedrive-clean\***: Curated stakeholder views (consultants, contractors, ILBANK) aligned with DirectoryManagementSystem rules. Accepted file formats within this tree are restricted to `pdf`, `doc`, `docx`, `xls`, `xlsx`, `ppt`, `pptx`, `pps`, `ppsx`, `csv`, `vsdx`, `jpg`, `png`, `tiff`, `txt`, `rtf`, `md`, `mp4`, `mov`, `wmv`, `avi`, `avchd`, `flv`, `f4v`, and `swf`.
- **DataWarehouse\ai4hsse-clean\processed\***: Pipeline-controlled workspaces for consultants/contractors deliverables, regenerated by 02_processing and 03_rag stages. Machine-learning ready artifacts promoted here must be stored as UTF-8 `.txt` files to ensure downstream model compatibility.
- **DataWarehouse\ai4hsse-clean\analytics\***: Aggregated KPI and incident statistics consumed by dashboards.
- **DataWarehouse\ai4hsse-clean\rag_ready\chunks_parquet**: Final RAG chunks surfaced to the API layer.
- **C:\vscode-projects\ai4ohs-hybrid\***: Canonical repository root; all source code, scripts, and configuration must remain inside this tree.
- **C:\vscode-projects\ai4ohs-hybrid\logs\***: Centralized location for runtime logs and generated output artifacts produced by pipelines, sanitizers, and automation.
- **H:\DataLake\**: Authoritative storage for all raw datasets sourced via ingest workflows; no processed artifacts should be written here.
- **H:\DataWarehouse\**: Official landing zone for all processed, curated, or analytics-ready data products generated by pipeline stages.

## Naming Rules

1. **Language**: English names only. Replace locale-specific characters (`ç, ğ, ı, İ, ö, ş, ü`) with ASCII equivalents.
2. **Characters**: Allowable set is `[A-Za-z0-9_-]`. Spaces become underscores, consecutive separators collapse to one. Avoid trailing dots or spaces (NTFS restriction).
3. **Case**: Use lower_case_with_underscores for files, kebab-case for folders when readability improves (`safety-manuals`). Tenant-specific directories (e.g., `KMARAS-W5.4_NETWORKS_OZSU_BUGRA_TEKTAS_JV`) remain verbatim but must pass sanitizer checks.
4. **Versioning**: Append `_vYYYYMMDD` or `_vX.Y.Z` as needed; avoid using words like "final". Archive immutable snapshots under `_archive\{YYYYQ#}`.
5. **Timestamps**: ISO 8601 in filenames when required (e.g., `incident_report_2025-11-12.pdf`). Use partitioned folders (`year=YYYY\month=MM\day=DD`) for timeseries telemetry.

## Workflow

1. **Acquisition**
	- Operators drop HSSE documents into `H:\DataLake\ai4hsse-raw\00_sources\_dropzone`; market data feeds arrive via automated connectors into `ai4crypto-raw\raw`.
	- `scripts/dev/reorg_sanitizer.py` normalizes names, computes SHA-256 hashes, checks for duplicates, and routes payloads into MIME-specific folders (`hsse-docs`, `images`, `terrp-docs`).
	- System-maintained ingress logs accumulate under `ai4hsse-raw\_system\logs` and `ai4crypto-raw\logs\ingestion`.

2. **Curation**
	- Stage `01_staging` emits normalized text and OCR payloads into `H:\DataLake\ai4hsse-raw\01_staging` before promoting curated outputs to `H:\DataWarehouse\ai4hsse-clean\onedrive-clean`.
	- DirectoryManagementSystem policies apply to consultant, contractor, and ILBANK branches; lineage manifests live alongside deliverables.
	- For market telemetry, curated datasets (train/test splits, walk-forward windows) remain under `ai4crypto-raw\curated` with schema definitions in `metadata\schema_registry`.

3. **Processing**
	- Stage `02_processing` writes tokenized chunks, embeddings, and FAISS build inputs to `H:\DataLake\ai4hsse-raw\02_processing`, then promotes consolidated artifacts to `H:\DataWarehouse\ai4hsse-clean\processed` and `faiss`.
	- RAG-ready parquet shards land in `H:\DataWarehouse\ai4hsse-clean\rag_ready\chunks_parquet`; embeddings for ai4crypto workloads are stored in `ai4crypto-clean` per model/version.
	- Subdirectories are regenerated on every run; immutable versions move to `_archive` or `exports\reports` as needed.

4. **Access Control**
	- Apply NTFS ACLs: read-only for consumers on `onedrive-clean` and `exports`, restricted read/write for pipeline service accounts on `processed`, `analytics`, and `rag_ready`.
	- Pipeline automation accounts maintain `ai4hsse-raw\_system\backup` snapshots (daily/weekly/monthly) and checksum registries under `ai4crypto-raw\audits\checksums`.
	- Retain sanitizer activity in `logs/dev/sanitizer.log`; cross-check with `_system\_logs` for drift detection.

## DirectoryManagementSystem Checklist

- [ ] Enforce naming rules via automated validation script before upload (covers `ai4hsse-raw\00_sources` and `ai4hsse-clean\onedrive-clean`).
- [ ] Generate lineage manifest (`artifacts_manifest.json`) for each curated export or contractor delivery.
- [ ] Store SHA-256 hash registry under `ai4crypto-raw\audits\checksums` and mirror to `ai4hsse-clean\exports\reports\hashes\` for reproducibility.
- [ ] Run `scripts/dev/reorg_sanitizer.py` in watch mode during business hours and on-demand after bulk uploads.
- [ ] Review `logs/dev/sanitizer.log` and `_system\_logs` daily for anomalies; escalate discrepancies to DataOps.
- [ ] Validate `_archive` quarterly to ensure quarter folders contain immutable snapshots only.

## Maintenance Tasks

| Task | Frequency | Owner | Tool |
| --- | --- | --- | --- |
| Sanity check ">255 char" names | Weekly | DataOps | `scripts/tools/validate_tree.py` |
| Purge orphaned hashes | Monthly | ML Ops | `scripts/dev/zeus_recovery.py` |
| Snapshot `processed` artifacts | After pipeline runs | ML Ops | `robocopy` or `rclone` |
| Verify ACL compliance (`onedrive-clean`, `processed`, `_system`) | Quarterly | Security | Windows `icacls` audit |
| Rebuild `_system\backup\monthly` index | Monthly | Platform Ops | `PowerShell` automation |
| Validate `rag_ready\chunks_parquet` schema vs `documents_parquet` | After schema changes | ML Ops | `pytest -k rag_schema` |

## Future Enhancements

- Integrate `DirectoryManagementSystem` into Zeus sanitizer for real-time validation.
- Add PowerShell script to bootstrap required directory skeletons across `ai4crypto-raw` and `ai4hsse-raw` if missing.
- Implement checksum validation on pipeline start-up to detect unauthorized modifications.
- Extend `_system\_reports` with quarterly FFMP compliance dashboards.
